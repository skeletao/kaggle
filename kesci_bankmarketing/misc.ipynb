{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import orm\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import Column, Integer, String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyltp\n",
    "from pyltp import SentenceSplitter\n",
    "from pyltp import Segmentor\n",
    "from pyltp import Postagger\n",
    "from pyltp import NamedEntityRecognizer\n",
    "from pyltp import Parser\n",
    "from pyltp import SementicRole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from DB server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 数据库地址（Host）rm-8vbwj6507z6465505ro.mysql.zhangbei.rds.aliyuncs.com\n",
    "- 用户名（User）：root\n",
    "- 用户密码（Password）：AI@2019@ai\n",
    "- 数据库名（Database）：stu_db\n",
    "- 表名：news_chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_url = {\n",
    "    'database': 'stu_db',\n",
    "    'drivername': 'mysql+mysqldb',\n",
    "    'username': 'root',\n",
    "    'password': 'AI@2019@ai',\n",
    "    'host': 'rm-8vbwj6507z6465505ro.mysql.zhangbei.rds.aliyuncs.com',\n",
    "    'query': {'charset': 'utf8'},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mysql+mysqldb://root:***@rm-8vbwj6507z6465505ro.mysql.zhangbei.rds.aliyuncs.com/stu_db?charset=utf8"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = URL(**db_url)\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(url, echo=True, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-16 19:40:13,431 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'sql_mode'\n",
      "2019-08-16 19:40:13,434 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-08-16 19:40:13,542 INFO sqlalchemy.engine.base.Engine SELECT DATABASE()\n",
      "2019-08-16 19:40:13,545 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-08-16 19:40:13,660 INFO sqlalchemy.engine.base.Engine show collation where `Charset` = 'utf8' and `Collation` = 'utf8_bin'\n",
      "2019-08-16 19:40:13,662 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-08-16 19:40:13,729 INFO sqlalchemy.engine.base.Engine SELECT CAST('test plain returns' AS CHAR(60)) AS anon_1\n",
      "2019-08-16 19:40:13,732 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-08-16 19:40:13,790 INFO sqlalchemy.engine.base.Engine SELECT CAST('test unicode returns' AS CHAR(60)) AS anon_1\n",
      "2019-08-16 19:40:13,794 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-08-16 19:40:13,849 INFO sqlalchemy.engine.base.Engine SELECT CAST('test collated returns' AS CHAR CHARACTER SET utf8) COLLATE utf8_bin AS anon_1\n",
      "2019-08-16 19:40:13,852 INFO sqlalchemy.engine.base.Engine ()\n"
     ]
    }
   ],
   "source": [
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-16 19:40:27,343 INFO sqlalchemy.engine.base.Engine show tables\n",
      "2019-08-16 19:40:27,345 INFO sqlalchemy.engine.base.Engine ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception during reset or similar\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sqlalchemy\\pool.py\", line 709, in _finalize_fairy\n",
      "    fairy._reset(pool)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sqlalchemy\\pool.py\", line 880, in _reset\n",
      "    pool._dialect.do_rollback(self)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sqlalchemy\\dialects\\mysql\\base.py\", line 1796, in do_rollback\n",
      "    dbapi_connection.rollback()\n",
      "_mysql_exceptions.OperationalError: (2006, 'MySQL server has gone away')\n"
     ]
    }
   ],
   "source": [
    "tables = connection.execute('show tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-16 19:41:18,564 INFO sqlalchemy.engine.base.Engine show full columns from news_chinese\n",
      "2019-08-16 19:41:18,566 INFO sqlalchemy.engine.base.Engine ()\n",
      "('id', 'int(11)', None, 'NO', 'PRI', None, '', 'select,insert,update,references', '')\n",
      "('author', 'varchar(32)', 'utf8_general_ci', 'YES', '', None, '', 'select,insert,update,references', '')\n",
      "('source', 'varchar(32)', 'utf8_general_ci', 'YES', '', None, '', 'select,insert,update,references', '')\n",
      "('content', 'varchar(1000)', 'utf8_general_ci', 'YES', '', None, '', 'select,insert,update,references', '')\n",
      "('feature', 'varchar(256)', 'utf8_general_ci', 'YES', '', None, '', 'select,insert,update,references', '')\n",
      "('title', 'varchar(32)', 'utf8_general_ci', 'YES', '', None, '', 'select,insert,update,references', '')\n",
      "('url', 'varchar(32)', 'utf8_general_ci', 'YES', '', None, '', 'select,insert,update,references', '')\n"
     ]
    }
   ],
   "source": [
    "fileds = connection.execute('show full columns from news_chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-16 19:43:15,663 INFO sqlalchemy.engine.base.Engine select count(*) from news_chinese\n",
      "2019-08-16 19:43:15,665 INFO sqlalchemy.engine.base.Engine ()\n",
      "(89611,)\n"
     ]
    }
   ],
   "source": [
    "count = connection.execute('select count(*) from news_chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-16 19:44:36,633 INFO sqlalchemy.engine.base.Engine select * from news_chinese limit 5\n",
      "2019-08-16 19:44:36,635 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-08-16 19:44:36,771 INFO sqlalchemy.engine.base.Engine select * from news_chinese_backup limit 5\n",
      "2019-08-16 19:44:36,773 INFO sqlalchemy.engine.base.Engine ()\n"
     ]
    }
   ],
   "source": [
    "query = 'select * from news_chinese limit 5'\n",
    "df = pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['id', 'author', 'source', 'content', 'feature', 'title', 'url'], dtype='object'),\n",
       " RangeIndex(start=0, stop=5, step=1))"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns, df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"type\":\"体育\",\"site\":\"新华社\",\"url\":\"http://home.xinhua-news.com/gdsdetailxhsnew/2203534-?pageflag=init&imageOrVedioTypeGdsId=\",\"site_sign\":\"xinhua\",\"keywords\":\"乒乓球\",\"summary\":\"\\u3000\\u3000新华社德国杜塞尔多夫６月６日电 题：乒乓女球迷\\u3000 \\u3000\\u3000新华社记者王子江、张寒 \\u3000\\u3000熊老师离开上海前，特意花一千多元买了一只张继科代言的球拍，准备在世界锦标赛期间他\"}'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_id1 = df.loc[0, 'feature']+ r'\"}'\n",
    "df_id1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keywords': '乒乓球',\n",
       " 'site': '新华社',\n",
       " 'site_sign': 'xinhua',\n",
       " 'summary': '\\u3000\\u3000新华社德国杜塞尔多夫６月６日电 题：乒乓女球迷\\u3000 \\u3000\\u3000新华社记者王子江、张寒 \\u3000\\u3000熊老师离开上海前，特意花一千多元买了一只张继科代言的球拍，准备在世界锦标赛期间他',\n",
       " 'type': '体育',\n",
       " 'url': 'http://home.xinhua-news.com/gdsdetailxhsnew/2203534-?pageflag=init&imageOrVedioTypeGdsId='}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_id1 = json.loads(df_id1)\n",
    "df_id1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-16 21:49:26,250 INFO sqlalchemy.engine.base.Engine select * from news_chinese\n",
      "2019-08-16 21:49:26,259 INFO sqlalchemy.engine.base.Engine ()\n"
     ]
    }
   ],
   "source": [
    "query = 'select * from news_chinese'\n",
    "df = pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('news_chinese.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class News(Base):\n",
    "    __tablename__ = 'news_chinese'\n",
    "     \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    author = Column(String)\n",
    "    source = Column(String)    \n",
    "    content = Column(String)    \n",
    "    feature = Column(String)   \n",
    "    title = Column(String)        \n",
    "    url = Column(String)     \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"<news_chinese(id='%s', author='%s', title='%s')>\" % (self.id, self.author, self.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-16 20:08:09,589 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)\n",
      "2019-08-16 20:08:09,592 INFO sqlalchemy.engine.base.Engine SELECT news_chinese.id AS news_chinese_id, news_chinese.author AS news_chinese_author, news_chinese.source AS news_chinese_source, news_chinese.content AS news_chinese_content, news_chinese.feature AS news_chinese_feature, news_chinese.title AS news_chinese_title, news_chinese.url AS news_chinese_url \n",
      "FROM news_chinese\n",
      "2019-08-16 20:08:09,594 INFO sqlalchemy.engine.base.Engine ()\n"
     ]
    }
   ],
   "source": [
    "news_all = session.query(News).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\n新华社北京6月4日电（记者徐力宇）叙利亚库尔德人武装“人民保护部队”一名发言人3日说，夺取极端组织“伊斯兰国”位于叙北部的大本营拉卡的总攻将在“几天后”开始。\\\\n【强弩之末】\\\\n\\\\n2017年5月18日，在叙利亚拉卡附近的塔卜卡镇，两名“叙利亚民主军”士兵走在街道上。（新华/法新）\\\\n“人民保护部队”是叙利亚反对派武装“叙利亚民主军”的主力，自去年11月以来，“叙利亚民主军”已经从多方包围了拉卡。\\\\n在伊拉克，摩苏尔战役已经进入最后关头，“伊斯兰国”控制的地盘所剩无几，在叙利亚也是节节败退，拉卡之役将进一步削弱“伊斯兰国”力量。\\\\n“武装力量已经推进到拉卡市郊区，总攻将在几天后开始。”“人民保护部队”发言人努里·马哈茂德在电话采访中告诉路透社记者。\\\\n拉卡位于幼发拉底河沿岸，距离土耳其边境约90公里。此前有媒体报道，“叙利亚民主军”的发言人也表示，拉卡之役下一阶段战斗将在“几天后”开始。\\\\n由美国主导的打击“伊斯兰国”联盟发言人瑞安·狄龙拒绝置评收复拉卡行动下一阶段时间表。他说，“叙利亚民主军”每天都在向前推进，在北面和东面离拉卡市区只有3公里，在西面推进到离市区不到10公里处。\\\\n【武器到手】\\\\n\\\\n2017年5月12日，在叙利亚北部塔卜卡，“叙利亚民主军”士兵走过街头。（新华/路透）\\\\n\\\\n“叙利亚民主军”由叙利亚阿拉伯人和库尔德人组成，美国把其中的库尔德人武装“人民保护部队”视为战斗力最强的一支，并提供空中掩护，派遣军事顾问，帮助他们对抗“伊斯兰国”。\\\\n美国国防部上月宣布，总统唐纳德·特朗普已经批准向“人民保护部队”提供火力更强的武器，以确保夺下拉卡。美国军方5月30日透露，已经开始向这支武装分发武器。\\\\n土耳其强烈反对“人民保护部队”获得美援，原因是土耳其把这支库尔德人武装视为库尔德工人党的分支。库尔德工人党成立于1979年，多年来试图以武力在土耳其与伊拉克、伊朗、叙利亚交界处的库尔德人聚居区建立独立国家，被土耳其、美国和欧盟认定为恐怖组织。土耳其已经派兵进入叙北部，试图阻止“人民保护部队”扩大势力范围。\\\\n打击“伊斯兰国”联盟认为，目前还有3000到4000名“伊斯兰国”武装人员盘踞在拉卡市内，并筑起防御工事。\\\\n“战斗不会轻松，”马哈茂德说，“‘伊斯兰国’确实有地道、地雷、汽车炸弹、自杀式袭击者，同时他们还把平民当作人盾。”\\\\n'"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_all[3458].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-16 20:07:51,802 INFO sqlalchemy.engine.base.Engine ROLLBACK\n"
     ]
    }
   ],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data directly from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('C:/Users/7153678/Desktop/AI/src/nlp/data/news_chinese2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_nona = news.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_cnt = news_nona.iloc[:, 1:].applymap(len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_cnt.insert(0, 'id', news_nona.values[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                     87307\n",
       "author                                              廖越 PF062\n",
       "source                                                  凤凰财经\n",
       "content                                                ?\\r\\n\n",
       "feature    {\"type\":\"财经\",\"site\":\"凤凰\",\"commentNum\":\"0\",\"joi...\n",
       "title                                       一张图看懂一带一路的钱都从哪来？\n",
       "url        http://finance.ifeng.com/a/20170515/15374639_0...\n",
       "Name: 2310, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_nona[~news_nona['content'].str.strip(r'\\\\n').str.contains(r'\\w', regex=True)].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sorted = news_nona.assign(f = news_nona['content'].str.len()).sort_values('f', ascending = True).drop('f', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_big_content = news_sorted[news_sorted['content'].str.len()>=100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_web_chars(ss, chars=r'\\\\n|&nbsp|\\xa0|\\\\xa0|\\u3000|\\\\u3000|\\\\u0020|\\u0020'):\n",
    "    if isinstance(ss, str):\n",
    "        return re.sub(chars, '', ss) \n",
    "    else:\n",
    "        return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_final = news_big_content.applymap(clear_web_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新华社曼谷５月８日电（记者杨舟）“泰国－香港－上海，‘一带一路’下的战略伙伴”研讨会８日在曼谷举行。\n",
      "泰国副总理颂奇表示泰国希望将“东部经济走廊”建设与中国“一带一路”倡议对接，并邀请中国企业家来泰投资助力泰国发展。\n",
      "颂奇在研讨会上作主旨演讲时说，英国“脱欧”等事件出现后，世界充满不确定性，自由贸易受到挑战，中国提出的“一带一路”倡议成为“新希望”。\n",
      "他称泰国十分重视即将举行的“一带一路”国际合作高峰论坛，将派６位部长参加。\n",
      "颂奇说，泰国现在大力发展“东部经济走廊”，即通过建设高铁、机场、深水港等基础设施，促进新能源汽车、生物科技、医疗保健、食品加工等高附加值新产业发展，以让泰国成为东盟，至少是中南半岛的中心，希望港、沪两地的企业家帮助实现“东部经济走廊”发展，实现泰国与“一带一路”倡议的对接。\n",
      "香港贸易发展局主席罗康瑞说，促进“一带一路”倡议已经成为香港发展的动力，而泰国政府提出的“东部经济走廊”许多方面都与“一带一路”倡议有契合之处，香港也希望利用优势与泰国加强在“一带一路”倡议下的合作。\n",
      "上海市人民政府港澳事务办公室副主任周亚军表示，上海市在建设金融、航运、科技创新中心等方面积累了经验，上海企业在基础设施建设上水平先进，本次带领上海企业来参加研讨会，就是希望寻找与泰国在“一带一路”倡议下的合作机会。\n",
      "本次研讨会由泰国商业部、工业部、香港贸易发展局和上海市人民政府港澳事务办公室共同举办，有近３００位来自政府和企业的代表参会。\n",
      "（完）\n"
     ]
    }
   ],
   "source": [
    "sents = SentenceSplitter.split(news_final.iloc[56701, 3])\n",
    "print('\\n'.join(sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar words by W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "W2V_DATA_DIR = r'C:\\Users\\7153678\\Desktop\\AI\\src\\nlp\\model\\word2vec'  # word2vec模型目录的路径, trained by wiki\n",
    "w2v_model_path = os.path.join(W2V_DATA_DIR, 'word2vec_wiki.model')\n",
    "model = Word2Vec.load(w2v_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeate most_similar has better performance\n",
    "    # if s in seen:\n",
    "    #     seen[s] = seen[s]+1\n",
    "    # else:\n",
    "    #     candidates.append(s)\n",
    "# most_similar length 20 is better than 10\n",
    "\n",
    "# To do...\n",
    "# optimal: 1. score function could be revised, use function of tree level and distance to root to replace seen[oldest] += ?\n",
    "# optimal: 2. using dymanic programming to reduce computing time  \n",
    "\n",
    "def get_similar_words(word, num, model):\n",
    "    \n",
    "    seen = defaultdict(int) \n",
    "    candidates = [word]   \n",
    "    \n",
    "    while len(seen) < num and candidates:\n",
    "        \n",
    "        if len(seen) % 50 == 0: \n",
    "            print('seen length : {}'.format(len(seen)))\n",
    "            \n",
    "        oldest = candidates.pop(0)\n",
    "        similars = model.most_similar(oldest, topn = 20)   \n",
    "        \n",
    "        candidates += [s for s, p in similars]       \n",
    "        seen[oldest] += 1\n",
    "        \n",
    "#         for s, p in similars:\n",
    "#             if s in seen:\n",
    "#                 seen[s] = seen[s]+1\n",
    "#             else:\n",
    "#                 candidates.append(s)\n",
    "#         seen[oldest] = 1\n",
    " \n",
    "    #similar_words = pd.DataFrame(list(seen.items()), columns=['word', 'count']).sort_values('count', ascending=False)\n",
    "    similar_words = sorted(seen.items(), key=lambda x: x[1], reverse=True)\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen length : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen length : 50\n",
      "seen length : 100\n",
      "seen length : 150\n",
      "seen length : 200\n",
      "seen length : 250\n",
      "seen length : 300\n",
      "seen length : 350\n",
      "seen length : 400\n",
      "seen length : 400\n",
      "seen length : 450\n"
     ]
    }
   ],
   "source": [
    "similar_words = get_similar_words('说',500,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "says = similar_words[0:50]\n",
    "says = [word[0] for word in says]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pylpt to parse sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTP_DATA_DIR = 'C:/Users/7153678/Desktop/AI/src/nlp/model/ltp_data'  # ltp模型目录的路径\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')\n",
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  # 词性标注模型路径，模型名称为`pos.model`\n",
    "ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')  # 命名实体识别模型路径，模型名称为`pos.model`\n",
    "par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')  # 依存句法分析模型路径，模型名称为`parser.model`\n",
    "srl_model_path = os.path.join(LTP_DATA_DIR, 'srl')  # 语义角色标注模型目录路径，模型目录为`srl`。注意该模型路径是一个目录，而不是一个文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import jieba\n",
    "\n",
    "def cut(ss):\n",
    "    ss_w = ''.join(re.findall(r'\\w+', ss))\n",
    "    return ' '.join(jieba.cut(ss_w))\n",
    "\n",
    "def get_tfidf(sents):\n",
    "    corpus = [cut(ss) for ss in sents]\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_distance(v1, v2, factor=0.8):\n",
    "    #euclidean distance\n",
    "    dist = np.linalg.norm(v1-v2)\n",
    "    #cosine similarity\n",
    "    cos = 1 -np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    return factor*dist+(1-factor)*cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence(sentence):\n",
    "    # cur words\n",
    "    segmentor = Segmentor()\n",
    "    segmentor.load(cws_model_path)\n",
    "    words = segmentor.segment(sentence)\n",
    "    # print(' '.join(words))\n",
    "    segmentor.release()\n",
    "    \n",
    "    # postag\n",
    "    postagger = Postagger()\n",
    "    postagger.load(pos_model_path)\n",
    "    postags = postagger.postag(words)\n",
    "    # print(' '.join(postags))\n",
    "    postagger.release()\n",
    "    \n",
    "    # NER\n",
    "    nerecognizer = NamedEntityRecognizer()\n",
    "    nerecognizer.load(ner_model_path)\n",
    "    nerags = nerecognizer.recognize(words, postags)\n",
    "    # print(' '.join(nerags))\n",
    "    nerecognizer.release()\n",
    "    \n",
    "    # dependency\n",
    "    parser = Parser()\n",
    "    parser.load(par_model_path)\n",
    "    arcs = parser.parse(words, postags)\n",
    "    # print(' '.join('%d:%s %s<--%s ' % (arc.head, arc.relation, words[i], words[arc.head-1]) for i, arc in enumerate(arcs)))\n",
    "    parser.release()\n",
    "    \n",
    "    return words, postags,nerags, arcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opinion(content, says):\n",
    "    name_opinion = {}\n",
    "    sents = SentenceSplitter.split(content)\n",
    "    #print('\\n'.join(sents))\n",
    "    sent_vectors = get_tfidf(sents)\n",
    "    for i in range(len(sents)):      \n",
    "        j = i\n",
    "        person = ''\n",
    "        opinion = ''\n",
    "        \n",
    "        #parse each entence as DF\n",
    "        words, postags, nerags, arcs = parse_sentence(sents[i])\n",
    "        relation_index = [[i,  words[i], postags[i], nerags[i], arc.relation, words[arc.head-1]] for i, arc in enumerate(arcs)]\n",
    "        df_relataion = pd.DataFrame(relation_index, columns=['index', 'word', 'postag', 'ner', 'relation', 'relate_word'])\n",
    "        hed_word = df_relataion[df_relataion['relation']=='HED']['word'].iloc[0]\n",
    "\n",
    "        #check if say word and alid person exist\n",
    "        if not hed_word in says:\n",
    "            continue\n",
    "        df_names = df_relataion.loc[(df_relataion['relation']=='SBV') & (df_relataion['relate_word']==hed_word)]\n",
    "        for i in range(len(df_names)):\n",
    "            ner = df_names.iloc[i]['ner']\n",
    "            if ner in ['S-Nh', 'B-Nh', 'I-NH', 'E-Nh']:\n",
    "                person += df_names.iloc[i]['word']\n",
    "        if not person:\n",
    "            continue\n",
    "        \n",
    "        hed_index = df_relataion[df_relataion['relation']=='HED']['index'].iloc[0]\n",
    "\n",
    "        # say word is followed by “”\n",
    "        if ''.join(df_relataion.iloc[hed_index+1:hed_index+3]['word'].tolist())in [r'：“', r'，“']:\n",
    "            for sss in words[hed_index+1:]:\n",
    "                if sss == r'”':\n",
    "                    return \n",
    "                else:\n",
    "                    opinion += sss\n",
    "        if opinion and not opinion[-1]==r'。':\n",
    "            opinion += '。'\n",
    "                    \n",
    "        # no “”, just words            \n",
    "        start_index = hed_index+1\n",
    "        if df_relataion.loc[start_index]['postag'] == 'wp':\n",
    "            start_index += 1;               \n",
    "        opinion = ''.join(df_relataion['word'][start_index:])\n",
    "        if not df_relataion.iloc[-1]['postag'] == 'wp':\n",
    "            opinion += '。'\n",
    "\n",
    "        # say word is followed by ，。\n",
    "        if df_relataion.iloc[hed_index+1]['word'] in [r'，', r'。']:  \n",
    "            for iii, sss in enumerate(words[:hed_index]):\n",
    "                if sss == r'，':\n",
    "                    return \n",
    "                else:\n",
    "                    opinion += sss\n",
    "            if opinion and not opinion[-1]==r'。':\n",
    "                opinion += '。'\n",
    "            \n",
    "        #check if next sentence is still on saying.\n",
    "        similarity_with_next = mix_distance(sent_vectors[1,:].toarray()[0], sent_vectors[2,:].toarray()[0])\n",
    "        if similarity_with_next > 10:\n",
    "            opinion += sents[i+1]\n",
    "            i += 1  \n",
    "\n",
    "        print('person is', person)      \n",
    "        print('opinion is', opinion)\n",
    "\n",
    "        name_opinion[(j, person)] = opinion\n",
    "        df_opinion = pd.DataFrame([[key[0], key[1], val] for key, val in name_opinion.items()], columns=['index','name', 'opinion'])\n",
    "    return df_opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_new = news_final.iloc[66701, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\7153678\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.154 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person is 汪浚\n",
      "opinion is 希望能做到三件事：\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person is 刘汉文\n",
      "opinion is 了高度的肯定，在其看来，一个良性行业市场发展，除却政府政策推动和扶持以外，更需要市场自行的组织建设。\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>name</th>\n",
       "      <th>opinion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>汪浚</td>\n",
       "      <td>希望能做到三件事：</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>刘汉文</td>\n",
       "      <td>了高度的肯定，在其看来，一个良性行业市场发展，除却政府政策推动和扶持以外，更需要市场自行的组...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index name                                            opinion\n",
       "0     44   汪浚                                          希望能做到三件事：\n",
       "1     60  刘汉文  了高度的肯定，在其看来，一个良性行业市场发展，除却政府政策推动和扶持以外，更需要市场自行的组..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_opinion = get_opinion(one_new, says)\n",
    "df_opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果说，近期广电对于点播影院行业政策的出台——4月21日《国家新闻出版广电总局关于规范点播影院、点播院线经营管理工作的通知》发布、6月12日《点播影院、点播院线管理暂行规定》(征求意见稿)公示，宣告着整个行业的剧变，令行业惊呼“电影行业新纪元”的到来，那“一起看”携手“艾米”战略合作并联合发起成立“中国点播影院联盟”，那当真算得上是“风暴后的第一缕惊雷”。\n",
      "\n",
      "\n",
      "6月17日，“中国点播影院联盟发起大会暨一起看&艾米战略合作发布会”在北京召开，双方宣布将会携手400多家门店，在内容共享、设备共享、门店互通、会员互通等方面充分互动，并组建联盟吸纳行业同盟成员，以联合之力打造行业新格局。\n",
      "\n",
      "\n",
      "对点播影院行业而言，历经五年的市场培育与生长，政策的出台，无疑标志着行业正式被纳入官方视野，政府在扶持的同时，严控牌照等各项标准的界定，可以说一场颠覆的战役即将打响。\n",
      "谈起联盟的建立初衷，发起人之一的广州一起看信息科技有限公司董事长孙亚琼表示，希望通过联盟的方式，以联合之力获取行业更高效的发展。\n",
      "“一群无戒心而有信誉的朋友，更多的商业机会。”\n",
      "\n",
      "\n",
      "在其看来，过去的点播影院行业长期沉浸于设备厂商、系统商的“阴霾”当中，粗放式发展，以至于“假”的点播影院实在太多。\n",
      "\n",
      "\n",
      "大量由设备厂商入股的门店为买设备而开店，鼓吹资本力量和资金回报率，以至于行业乱象丛生，大量门店运营经验不足，在价位高于传统院线的情况下，却无法形成高端的品牌形象，高消费群体不愿意消费，消费惨淡，形成行业怪象。\n",
      "\n",
      "\n",
      "联盟之所以成立，除却抓住政策机遇之外，更在于整合行业间的有生力量，一则提高整体行业话语权，在版权、设备采购等多种方面以联合之力为联盟成员争取更大利益空间，避免以往相对被动的局面;\n",
      "二则，在于运营经验、资源技术等方面的分享，快速培育好整个市场，将“饼”做大，提高所有同业者的发展和生存空间，倡导行业自律，制定整体行业标准，让大家少走弯路，做“真”点播影院。\n",
      "\n",
      "\n",
      "“我自盛开、蝴蝶自来”孙亚琼用八个字总结了自己的梦想，而对于联盟的运营核心，其“协作产生效能，透明带来互信，分享产生价值，共荣产生未来。”\n",
      "四句法则显得掷地有声，而另一个发起人，艾米江苏数字电影文化发展有限公司CEO汪浚，则有着更为深刻直观的看法。\n",
      "\n",
      "\n",
      "在汪浚看来，去年中国电影市场虽然只实现了3.7%的增长，但其背后，却是去年传统影院终端的高速增长，影院场均人次实则明显下降，而导致这一结果的核心原因正是传统影院的发行机制——“通过渠道控制剥夺消费者的选择权”。\n",
      "\n",
      "\n",
      "“排期定生死”虽然稍显夸张，但对所有院线内容而言，发行窗口期的长短却直接决定了有多少人有机会看到，“你有排场不一定有票房，没排场一定没票房。”\n",
      "大量内容只能被动由影院方面排片决定命运，作为消费者，选择权却十分有限，《百鸟朝凤》主创下跪之事仿若昨日。\n",
      "\n",
      "\n",
      "伴随着政策出台，点播院线作为电影行业新兴力量正式入场，其由用户主导的点播属性，将被动选择变成反向发行，除却延长电影的盈利周期以外，更符合了用户的需求，正如我们现在的网络电视，不正是替代了过去电视“播啥看啥”的尴尬吗？\n",
      "\n",
      "\n",
      "相比于传统院线，点播影院蕴含着迥异于先辈的属性。\n",
      "相比传统院线的“一次性消费”，点播影院并不受时间限制，合理的价格、更加私密化、更加精致的观影环境，在消费升级的概念下，会让更多的观影群体向往。\n",
      "面对中国电影行业500亿市场规模的饕餮盛宴，点播影院的前景难道不会让人惊叹吗？\n",
      "\n",
      "\n",
      "此外，政策的出台，将私人影院整体定调为点播影院，不同于过于私人影院的的定位混乱，点播影院重点聚焦于影院，将对用户在心中构建品牌价值起到巨大的推动作用，未来点播影院的发展必然进入快车道。\n",
      "\n",
      "\n",
      "对于联盟建立的意义，汪浚将之一归结为三点：行业自律——是姿态也是竞争，资源共享——渠道利益最大化，产业合力——先把饼做大。\n",
      "而通过建立联盟，汪浚表示希望能做到三件事：\n",
      "\n",
      "其一，形成内容分账标准。\n",
      "通过联合之力去争取更高的分账比例与票价结构，通过最高的性价比去获取版权内容，以避免以往和版权方谈判时“谈低了片子拿不到，高了不赚钱”的尴尬;\n",
      "\n",
      "\n",
      "其二，推动技术规范统一，更高效地帮助联盟成员拿牌照以及运营效率。\n",
      "\n",
      "\n",
      "其三，联合内容营销。\n",
      "“举个例子，《摔跤吧爸爸》在国内市场取得如此高的成绩是事先难以预料的，其实在进入国内市场之前该片已然在国外上映半年以上，如果我们能联合起来，提前把这类影片引进来，你们可以想象一下场面。”\n",
      "\n",
      "\n",
      "在汪浚看来，除了提前引进片子引导市场，拓展营销收益之外，这种模式还将在用户心中形成更高的价值链，“原来点播影院还有这么好的东西，这是传统院线没有的”当这种印象形成一种观念，那将为整个行业带来极其巨大的发展。\n",
      "\n",
      "\n",
      "对于“一起看”和“艾米”牵手，国家新闻出版广电总局发展研究中心电影研究所所长刘汉文也表示了高度的肯定，在其看来，一个良性行业市场发展，除却政府政策推动和扶持以外，更需要市场自行的组织建设。\n",
      "“伴随着群众对电影消费要求更高，那影院也应该满足群众更加个性化的需求，希望中国点播联盟能推动行业走的更远，成为其他领域的典范”。\n",
      "\n",
      "\n",
      "圆桌对话环节，乐视、微影等企业大咖均到场激辩，针对中国点播影院行业的未来展开了深度交流和专业探讨。\n",
      "\n",
      "\n",
      "站在时代的风口浪尖上，当天时地利人齐至，抓住机遇你是那个时代的“弄潮儿”，行业领军携手共谋大蓝图，驭领中国点播影院新时代，相信由“一起看”和“艾米”联合发起的中国点播影院联盟定将成为行业的领路者，以联合之力撬动中国点播影院行业的健康发展快车道。\n",
      "\n",
      "\n",
      "对于中国电影产业的未来，我们有点话说!\n",
      "有些事干!\n",
      "而你，会是这场变革中的见证者吗？\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(SentenceSplitter.split(one_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\7153678\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.975 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((9, 463), 9)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xv = get_tfidf(sents)\n",
    "xv.shape, len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2746655158337354"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_distance(xv[1,:].toarray()[0], xv[2,:].toarray()[0], 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, postags, nerags, arcs = parse_sentence(sents[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_index = [[i,  words[i], postags[i], nerags[i], arc.relation, words[arc.head-1]] for i, arc in enumerate(arcs)]\n",
    "df_relataion = pd.DataFrame(relation_index, columns=['index', 'word', 'postag', 'ner', 'relation', 'relate_word'])\n",
    "hed_word = df_relataion[df_relataion['relation']=='HED']['word'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(df_relataion.iloc[2:4]['word'].tolist())=='发展局主席'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 香港\n",
      "1 贸易\n",
      "2 发展局\n",
      "3 主席\n",
      "4 罗康瑞\n"
     ]
    }
   ],
   "source": [
    " for iii, sss in enumerate(words[:5]):\n",
    "        print(iii,sss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_relataion.loc[start_index]['postag'] == 'wp': start_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'促进“一带一路”倡议已经成为香港发展的动力，而泰国政府提出的“东部经济走廊”许多方面都与“一带一路”倡议有契合之处，香港也希望利用优势与泰国加强在“一带一路”倡议下的合作。'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinions = ''.join(df_relataion['word'][start_index:])\n",
    "opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df_relataion.iloc[-1]['postag'] == 'wp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion = ''.join(df_relataion['word'][start_index:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S-Nh'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddd= df_relataion.loc[(df_relataion['relation']=='SBV') & (df_relataion['relate_word']==hed_word)]\n",
    "len(ddd)\n",
    "ddd.iloc[0]['ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4    S-Nh\n",
       " Name: ner, dtype: object, pandas.core.series.Series)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_names = ddd['ner']\n",
    "df_names, type(df_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'S-Nh' in df_names.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.series.Series, 0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddd= df_relataion.loc[(df_relataion['relation']=='SBV') & (df_relataion['relate_word'].isin(pd.Series('说shuokan')))]['word']\n",
    "type(ddd), len(ddd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
